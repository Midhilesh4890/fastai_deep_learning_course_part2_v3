{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Layer-Sequential Unit-Variance (LSUV) Weight Initialization\n",
    "\n",
    "#### Last Time\n",
    "In our [most recent notebook](https://github.com/jamesdellinger/fastai_deep_learning_course_part2_v3/blob/master) we throughly explored the world of neural network layer normalization. We paid special attention to batch normalization, first implementing it from scratch and then verifying that our home-grown attempt performed just as well as the built-in PyTorch batchnorm module.\n",
    "\n",
    "We then experienced first-hand batchnorm's inability to successfully handle training on very small mini-batch sizes. After trying out other proposed alternatives, including [layer norm](https://arxiv.org/abs/1607.06450), [instance norm](https://arxiv.org/abs/1607.08022), and [group norm](https://arxiv.org/abs/1803.08494), we saw how with the right set of tweaks batchnorm could actually work with small batch sizes, and outperform these other approaches.\n",
    "\n",
    "#### LSUV\n",
    "Now we turn our attention to layer-sequential unit-variance initialization. Developed in 2015 [by Mishkin and Matas](https://arxiv.org/abs/1511.06422), LSUV is particularly effective for ensuring that very deep networks with complex architectures have final layer outputs that are inside or close to a standard normal distribution. \n",
    "\n",
    "Below we outline each step necessary to perform LSUV, implement and perform the procedure on the same MNIST dataset we've been using in each of our notebooks so far in this course. We wrap up by observing how a network whose layer weights have been initialized by LSUV performs relative to a simple baseline CNN.\n",
    "\n",
    "#### Attribution\n",
    "Virtually all the code that appears in this notebook is the creation of [Sylvain Gugger](https://www.fast.ai/about/#sylvain) and [Jeremy Howard](https://www.fast.ai/about/#jeremy). The original version of this notebook that they made for the course lecture can be found [here](https://github.com/fastai/course-v3/blob/master/nbs/dl2/07a_lsuv.ipynb). I simply re-typed, line-by-line, the pieces of logic necessary to implement the functionality that their notebook demonstrated. In some cases I changed the order of code cells and or variable names so as to fit an organization and style that seemed more intuitive to me. Any and all mistakes are my own.\n",
    "\n",
    "On the other hand, all long-form text explanations in this notebook are solely my own creation. Writing extensive descriptions of the concepts and code in plain and simple English forces me to make sure that I actually understand how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from exports.nb_07 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin, as always, by grabbing the [MNIST](http://yann.lecun.com/exdb/mnist/) data and constructing a simple, 5-layer CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()\n",
    "x_train, x_valid = normalize_to(x_train, x_valid)\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "\n",
    "nh, bs = 50, 512\n",
    "c = y_train.max().item() + 1\n",
    "loss_func = F.cross_entropy\n",
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)\n",
    "\n",
    "mnist_view = view_tfm(1,28,28)\n",
    "cbfs = [Recorder,\n",
    "        partial(AvgStatsCallback, accuracy),\n",
    "        CudaCallback,\n",
    "        partial(BatchTransformXCallback, mnist_view)]\n",
    "\n",
    "# Number of output channels for each convolutional layer.\n",
    "n_outs = [8, 16, 32, 64, 64]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement the tweaks necessary to enable LSUV, we'll train a simple 5-layer CNN so we can have a baseline for comparison. We won't include batchnorm layers in this first, baseline CNN.\n",
    "\n",
    "Also, recall that our generalized ReLU class includes a hyperparameter called `sub`. This is a bias term that allows us to shift all our ReLU outputs up or down the y-axis, by a constant value. Although up until now we've used methods to define convolutional layers, this time we'll use a class so that we can have a property that stores the weights of a convolutional layer and another property that remembers the bias used to shift the ReLU activation layer outputs. Note, also, that our class includes a setter for the ReLU activation bias term. \n",
    "\n",
    "When we implement LSUV below it will become clear why we include all of these inside our convolutional layer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, n_in, n_out, ks=3, stride=2, sub=0., **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_in, n_out, ks, padding=ks//2, stride=stride, bias=True)\n",
    "        self.relu = GeneralRelu(sub=sub, **kwargs)\n",
    "        \n",
    "    def forward(self, x): return self.relu(self.conv(x))\n",
    "    \n",
    "    @property\n",
    "    def activation_bias(self): return -self.relu.sub\n",
    "    @activation_bias.setter\n",
    "    def activation_bias(self, v): self.relu.sub = -v\n",
    "    @property\n",
    "    def weight(self): return self.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(n_outs, data, 0.6, ConvLayer, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [2.22107921875, tensor(0.2165, device='cuda:0')]\n",
      "valid: [2.006489453125, tensor(0.3546, device='cuda:0')]\n",
      "train: [0.964456328125, tensor(0.6708, device='cuda:0')]\n",
      "valid: [0.2753438232421875, tensor(0.8988, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our baseline validation accuracy score of `89%`, we're ready to implement LSUV!\n",
    "\n",
    "Layer-Sequential Unit-Variance was introduced by Mishkin and Matas in their 2015 paper *[All You Need is a Good Init](https://arxiv.org/abs/1511.06422)* as a method for neural net layer weight initialization. The goal is to scale layer weights such that the variances of outputs from every layer are all 1.\n",
    "\n",
    "Here's the process:\n",
    "1. Initialize weights. The paper recommends orthonormal matrices. However, given that 2015 was quite a while ago in terms of \"deep learning years,\" we will use the default init method for PyTorch `nn.Conv2d`, which is Kaiming uniform.\n",
    "* Pass one batch through the model and record the outputs of all fully-connected linear and convolutional layers.\n",
    "* Rescale the weights of each layer according by the standard deviation of that layer's outputs.\n",
    "* Subtract the mean of each layer's activation outputs from the bias term of its ReLU activation function (using the `activation_bias` property setter we placed in the above `Conv_Layer` class) to ensure that activation outputs become normalized.\n",
    "* Repeat the above two steps over and over until the mean and variance of each layer's outputs are sufficiently close to 0 and 1, respectively.\n",
    "* Weights have been initialized and it's now time to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(n_outs, data, 0.6, ConvLayer, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Pass one batch through network and record linear and convolutional layer outputs\n",
    "\n",
    "Here's a helper method that retrieves one batch from our datasets' dataloaders. The `begin_batch()` method is executed so that all preprocessing callbacks will be called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_batch(dl, run):\n",
    "    run.xb, run.yb = next(iter(dl))\n",
    "    for cb in run.cbs: cb.set_runner(run)\n",
    "    run('begin_batch')\n",
    "    return run.xb, run.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(data.train_dl, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're only interested in the outputs of each convolutional or fully-connected linear layer, we'll create a function to recurse through our model's architecture, layer by layer, and return a list of all layers that meet this criteria.\n",
    "\n",
    "Note that `sum(<list object>, [])` concatenates all the elements of all lists we pass to it, beginning with the initial state of the final list argument. This allows us to recurse as many times as necessary, and return a single-level, non-nested list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_modules(module, condition):\n",
    "    if condition(module): return [module]\n",
    "    return sum([find_modules(o, condition) for o in module.children()], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network only has five convolutional layers, and no fully-connected linear layers, so we want to add each of these five layers to our list of modules to keep track of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([ConvLayer(\n",
       "    (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (relu): GeneralRelu()\n",
       "  ), ConvLayer(\n",
       "    (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): GeneralRelu()\n",
       "  ), ConvLayer(\n",
       "    (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): GeneralRelu()\n",
       "  ), ConvLayer(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): GeneralRelu()\n",
       "  ), ConvLayer(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): GeneralRelu()\n",
       "  )], 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules = find_modules(learn.model, lambda x: isinstance(x, ConvLayer))\n",
    "modules, len(modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will register hooks at each of the above five layers. This helper function will be used by the hooks to save the mean and standard deviation of the outputs at each layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(hook, module, inp, outp):\n",
    "    d = outp.data\n",
    "    hook.mean, hook.var, hook.std = d.mean().item(), d.var().item(), d.std().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the mean and standard deviation of each layer's outputs after running one batch through the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3632296919822693 0.45626315474510193 0.6754725575447083\n",
      "0.392016738653183 0.5587514042854309 0.7474967837333679\n",
      "0.32132089138031006 0.3610764741897583 0.600896418094635\n",
      "0.3329537510871887 0.30177319049835205 0.5493388772010803\n",
      "0.21434631943702698 0.10931219160556793 0.33062395453453064\n"
     ]
    }
   ],
   "source": [
    "with ForwardHooks(modules, append_stats) as hooks:\n",
    "    model(xb)\n",
    "    for hook in hooks: print(hook.mean, hook.var, hook.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps 3, 4, 5: Scale each layer's weights until layer outputs are normalized\n",
    "\n",
    "1. We first divide each layer's weights by the standard deviation of that layer's outputs.\n",
    "* Then we adjust the bias term of each layer's ReLU activation function by the mean of that layer's outputs.\n",
    "\n",
    "We repeat the above two steps over and over, until the each layer's outputs have a mean close to 0 and variance close to 1. We'll define a method just below to carry out this procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lsuv_module(module, xb):\n",
    "    h = ForwardHook(module, append_stats)\n",
    "    \n",
    "    while model(xb) is not None and abs(h.var - 1) > 1e-3: module.weight.data /= h.std\n",
    "    while model(xb) is not None and abs(h.mean)    > 1e-3: module.activation_bias -= h.mean\n",
    "    \n",
    "    h.remove()\n",
    "    return h.mean, h.var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using inputs from our single batch, we execute this process on each of our model's five convolutional layers, in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-3.9229586690225915e-08, 0.9999997019767761)\n",
      "(1.277242400732348e-08, 1.0000001192092896)\n",
      "(-1.862645149230957e-09, 0.9999998807907104)\n",
      "(-2.3748725652694702e-08, 1.000000238418579)\n",
      "(-1.862645149230957e-08, 0.9999997019767761)\n"
     ]
    }
   ],
   "source": [
    "for m in modules: print(lsuv_module(m, xb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model's convolutional layer weights have now been initialized and it's time to see if LSUV helps us get a better performance than the validation accuracy score of `89%` that we achieved with our baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.442014140625, tensor(0.8601, device='cuda:0')]\n",
      "valid: [0.159346337890625, tensor(0.9510, device='cuda:0')]\n",
      "train: [0.1893918359375, tensor(0.9415, device='cuda:0')]\n",
      "valid: [0.08857901611328126, tensor(0.9718, device='cuda:0')]\n",
      "CPU times: user 3.06 s, sys: 879 ms, total: 3.94 s\n",
      "Wall time: 3.94 s\n"
     ]
    }
   ],
   "source": [
    "%time run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of LSUV here is quite impressive: our validation accuracy score jumps almost 10% to just over `97%`! We see that LSUV helps our network converge much sooner.\n",
    "\n",
    "In general, LSUV is especially effective for complex and deeper network architectures, where it becomes increasingly difficult to initialize layer weights and still get unit variance at the final layer's activation outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "{\n",
       "const ip = IPython.notebook\n",
       "if (ip) {\n",
       "    ip.save_notebook()\n",
       "    console.log('a')\n",
       "    const s = `!python notebook2script_my_reimplementation.py ${ip.notebook_name}`\n",
       "    if (ip.kernel) { ip.kernel.execute(s) }\n",
       "}\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_auto_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/09_optimizers_my_reimplementation.ipynb

from exports.nb_08 import *

imagenette_url = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette-160'


# Helper method that takes a list of functions and applies
# them, in order, to an item in the dataset. **kwargs allows us
# to pass a group of hyperparameters from the hyperparameter
# dict as arguments for each func.
def apply_functions(x, funcs, **kwargs):
    for func in listify(funcs):
        x = func(x, **kwargs)
    return x

def sgd_step(param, lr, **kwargs):
    param.data.add_(-lr, param.grad.data)
    return param

class Recorder(Callback):
    def begin_fit(self): self.lrs, self.losses = [], []

    def after_batch(self):
        if not self.in_train: return
        self.lrs.append(self.opt.hyperparam_dicts[-1]['lr'])
        self.losses.append(self.loss.detach().cpu())

    def plot_lr(self):
        plt.xlabel('Iteration')
        plt.ylabel('Learning Rate')
        plt.title('Learning Rate')
        plt.plot(self.lrs)

    def plot_loss(self):
        plt.xlabel('Iteration')
        plt.ylabel('Train Loss')
        plt.title('Training Loss')
        plt.plot(self.losses)

    def plot(self, skip_last=0):
        losses=[o.item() for o in self.losses]
        n = len(losses) - skip_last
        plt.xscale('log')
        plt.xlabel('Learning Rate')
        plt.ylabel('Train Loss')
        plt.title('Learning Rate Finder')
        plt.plot(self.lrs[:n], self.losses[:n])

class ParamScheduler(Callback):
    _order=1
    def __init__(self, param_name, sched_funcs):
        self.param_name, self.sched_funcs = param_name, listify(sched_funcs)

    def begin_batch(self):
        if not self.in_train: return
        fs = self.sched_funcs
        if len(fs)==1: fs = fs * len(self.opt.param_groups)
        pos = self.n_epochs/self.epochs
        for f,h in zip(fs, self.opt.hyperparam_dicts): h[self.param_name] = f(pos)

class LR_Find(Callback):
    _order=1
    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):
        self.max_iter, self.min_lr, self.max_lr = max_iter, min_lr, max_lr
        self.best_loss = 1e9

    def begin_batch(self):
        if not self.in_train: return
        pos = self.n_iter/self.max_iter
        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos
        for h in self.opt.hyperparam_dicts: h['lr'] = lr

    def after_step(self):
        if self.n_iter >= self.max_iter or self.loss > self.best_loss * 10:
            raise CancelTrainException()
        if self.loss < self.best_loss: self.best_loss = self.loss

def weight_decay(weight, lr, wd, **kwargs):
    weight.data.mul(1 - lr*wd)
    return weight

weight_decay._defaults = dict(wd=0.)

def l2_reg(weight, lr, wd, **kwargs):
    weight.grad.data.add_(wd, weight.data)
    return weight

l2_reg._defaults = dict(wd=0.)

def maybe_update(steppers, defaults, get_defaults):
    for stepper in steppers:
        for key, value in get_defaults(stepper).items():
            if key not in defaults: defaults[key] = value

def get_defaults(stepper): return getattr(stepper, '_defaults', {})

class Optimizer():
    def __init__(self, param_groups, steppers, **defaults):

        # Get all the steppers
        self.steppers = listify(steppers)

        # Update stepper hyperparam defaults if necessary
        maybe_update(self.steppers, defaults, get_defaults)

        # param_groups could be a generator
        self.param_groups = list(param_groups)
        # ensure param_groups is a list of lists
        if not isinstance(self.param_groups[0], list):
            self.param_groups = [self.param_groups]
        self.hyperparam_dicts = [{**defaults} for param_group in self.param_groups]

    def grad_params(self):
        return [(param, hyperparam_dict) for param_group, hyperparam_dict in zip(self.param_groups, self.hyperparam_dicts)
               for param in param_group if param.grad is not None]

    def zero_grad(self):
        for param, hyperparam_dict in self.grad_params():
            param.grad.detach_()
            param.grad.zero_()

    def step(self):
        for param, hyperparam_dict in self.grad_params():
            apply_functions(param, self.steppers, **hyperparam_dict)

sgd_opt = partial(Optimizer, steppers=[weight_decay, sgd_step])

class Statistic():
    _defaults = {}
    def init_state(self, weight): raise NotImplementedError
    def update(self, weight, state, **kwargs): raise NotImplementedError

class StatefulOptimizer(Optimizer):
    def __init__(self, param_groups, steppers, statistics=None, **defaults):
        self.statistics = listify(statistics)
        maybe_update(self.statistics, defaults, get_defaults)
        super().__init__(param_groups, steppers, **defaults)
        self.state = {}

    def step(self):
        for param, hyperparam_dict in self.grad_params():
            if param not in self.state:
                # Create a state for the param and call all statistics
                # in order to initialize it.
                self.state[param] = {}
                maybe_update(self.statistics, self.state[param], lambda o: o.init_state(param))
            state = self.state[param]
            for statistic in self.statistics:
                state = statistic.update(param, state, **hyperparam_dict)
            apply_functions(param, self.steppers, **state, **hyperparam_dict)
            self.state[param] = state

def momentum_step(weight, lr, grad_avg, **kwargs):
    # The line below that uses add_ is equivalent to performing
    # the following in place: weight.data -= (lr * grad_avg)
    weight.data.add_(-lr, grad_avg)
    return weight

def linear_combination(value1, value2, β):
    return β * value1 + (1 - β) * value2


# Adam part 1: Update the decaying, dampened average of
# previous gradients.
class AverageGradients(Statistic):
    _defaults = dict(mom = 0.9)
    def __init__(self, dampening:bool = True): self.dampening = dampening
    def init_state(self, weight): return {'grad_avg': torch.zeros_like(weight.grad.data)}
    def update(self, weight, state, mom, **kwargs):
        state['mom_damp'] = 1 - mom if self.dampening else 1.
        state['grad_avg'].mul_(mom).add_(state['mom_damp'], weight.grad.data)
        return state

# Adam part 2: Update the decaying, dampened average of
# previous squared gradients.
class AverageSquaredGradients(Statistic):
    _defaults = dict(sqr_mom = 0.99)
    def __init__(self, dampening:bool = True): self.dampening = dampening
    def init_state(self, weight): return {'sqr_grad_avg': torch.zeros_like(weight.grad.data)}
    def update(self, weight, state, sqr_mom, **kwargs):
        state['sqr_mom_damp'] = 1 - sqr_mom if self.dampening else 1.
        # The addcmul_ operation performs an element-wise multiplication
        # of weight.grad.data and weight.grad.data, and then multiplies
        # this product by state['sqr_mom_damp']. This result is then
        # added to the result of state['sqr_grad_avg'].mul_(sqr_mom).
        state['sqr_grad_avg'].mul_(sqr_mom).addcmul_(state['sqr_mom_damp'], weight.grad.data, weight.grad.data)
        return state

# Adam part 3: Keep and update a count of the number of training
# iterations performed so far during all of training (across all
# mini-batches).
class StepCount(Statistic):
    def init_state(self, weight): return {'step': 0}
    def update(self, weight, state, **kwargs):
        state['step'] += 1
        return state


# Adam part 4: General method that can be used to get debias terms for
# the decaying, dampened average of previous gradients, as well as the
# decaying, dampened average of previous squared gradients.
def get_debiaser(mom, damp, step): return damp * (1 - mom**step) / (1 - mom)

def adam_step(weight, lr, mom, mom_damp, step, sqr_mom,
              sqr_mom_damp, grad_avg, sqr_grad_avg, eps, **kwargs):
    debias1 = get_debiaser(mom, mom_damp, step)
    debias2 = get_debiaser(sqr_mom, sqr_mom_damp, step)
    # The addcdiv operation performs the element-wise division of
    # grad_avg and ((sqr_grad_avg/debias2).sqrt() + eps). This result
    # is then multiplied by -lr/debias1, whose result is finally
    # added to weight.data.
    weight.data.addcdiv_(-lr/debias1, grad_avg, (sqr_grad_avg/debias2).sqrt() + eps)
    return weight

adam_step._defaults = dict(eps = 1e-5)

def adam_opt(extra_steppers=None, **kwargs): # Allow for inclusion of any other steppers
    return partial(StatefulOptimizer,
                   steppers=[adam_step, weight_decay] + listify(extra_steppers),
                   statistics=[AverageGradients(), AverageSquaredGradients(), StepCount()],
                   **kwargs)


# A function to get all of a model's parameter groups.
def param_getter(model): return model.parameters()

class Learner():
    def __init__(self, model, data, loss_func, opt_func=sgd_opt, lr=3e-4,
                 splitter=param_getter, callback_funcs=None):
        self.model, self.data, self.loss_func, self.opt_func, self.lr, self.splitter = model, data, loss_func, opt_func, lr, splitter
        self.in_train = False
        self.opt = None
        self.logger = print

        self.callbacks = []
        self.add_callback(TrainEvalCallback())
        self.add_callbacks(callback_func() for callback_func in listify(callback_funcs))

    def add_callbacks(self, callbacks):
        for callback in listify(callbacks): self.add_callback(callback)

    def add_callback(self, callback):
        callback.set_runner(self)
        setattr(self, callback.name, callback)
        self.callbacks.append(callback)

    def remove_callbacks(self, callbacks):
        for callback in listify(callbacks): self.callbacks.remove(callback)

    def one_batch(self, i, xb, yb):
        try:
            self.iter = i
            self.xb, self.yb = xb, yb
            self('begin_batch')
            self.pred = self.model(self.xb)
            self('after_pred')
            self.loss = self.loss_func(self.pred, self.yb)
            self('after_loss')
            if not self.in_train: return
            self.loss.backward()
            self('after_backward')
            self.opt.step()
            self('after_step')
            self.opt.zero_grad()
        except CancelBatchException:
            self('after_cancel_batch')
        finally:
            self('after_batch')

    def all_batches(self):
        self.iters = len(self.dl)
        try:
            for i, (xb, yb) in enumerate(self.dl): self.one_batch(i, xb, yb)
        except CancelEpochException: self('after_cancel_epoch')

    def do_begin_fit(self, epochs):
        self.epochs = epochs
        self.loss = tensor(0.)
        self('begin_fit')

    def do_begin_epoch(self, epoch):
        self.epoch = epoch
        self.dl = self.data.train_dl
        return self('begin_epoch')

    def fit(self, epochs, callback_funcs=None, reset_opt=False):
        # New feature: support passing callbacks directly to fit().
        # They are removed when fit is finished.
        self.add_callbacks(callback_funcs)
        # New feature: support creating an optimizer when fit() is
        # called. Replaces previously existing optimizer self.opt_func.
        if reset_opt or not self.opt:
            self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)

        try:
            self.do_begin_fit(epochs)
            for epoch in range(epochs):
                self.do_begin_epoch(epoch)
                if not self('begin_epoch'): self.all_batches()

                with torch.no_grad():
                    self.dl = self.data.valid_dl
                    # If self('begin_validate') hasn't been called
                    # by any callback, such as the TrainEvalCallback,
                    # then proceed to generate predictions for the
                    # entire validation set.
                    if not self('begin_validate'): self.all_batches()
                self('after_epoch')

        except CancelTrainException: self('after_cancel_train')
        finally:
            self('after_fit')
            self.remove_callbacks(callback_funcs)

    ALL_CALLBACKS = {'begin_batch',
                     'after_pred',
                     'after_loss',
                     'after_backward',
                     'after_step',
                     'after_cancel_batch',
                     'after_batch',
                     'after_cancel_epoch',
                     'begin_fit',
                     'begin_epoch',
                     'begin_validate',
                     'after_epoch',
                     'after_cancel_train',
                     'after_fit'}

    def __call__(self, callback_name):
        assert callback_name in self.ALL_CALLBACKS
        for callback in sorted(self.callbacks, key = lambda x: x._order):
            callback(callback_name)

class AvgStatsCallback(Callback):
    def __init__(self, metrics):
        self.train_stats, self.valid_stats = AvgStats(metrics,in_train=True), AvgStats(metrics,in_train=False)

    def begin_epoch(self):
        self.train_stats.reset()
        self.valid_stats.reset()

    def after_loss(self):
        stats = self.train_stats if self.in_train else self.valid_stats
        with torch.no_grad(): stats.accumulate(self.run)

    def after_epoch(self):
        self.logger(self.train_stats)
        self.logger(self.valid_stats)

def get_learner(n_outs, data, lr, layer, loss_func=F.cross_entropy,
                callback_funcs=None, opt_func=sgd_opt, **kwargs):
    model = get_cnn_model(data, n_outs, layer, **kwargs)
    init_cnn(model)
    return Learner(model, data, loss_func, lr=lr,
                   callback_funcs=callback_funcs, opt_func=opt_func)

import time
from fastprogress import master_bar, progress_bar
from fastprogress.fastprogress import format_time

class AvgStatsCallback(Callback):
    def __init__(self, metrics):
        self.train_stats, self.valid_stats = AvgStats(metrics,in_train=True), AvgStats(metrics,in_train=False)

    # Keep track of names of all items we're printing out.
    def begin_fit(self):
        metric_names = ['loss'] + [m.__name__ for m in self.train_stats.metrics]
        names = ['epoch'] + [f'train_{n}' for n in metric_names] + [
            f'valid_{n}' for n in metric_names] + ['time']
        self.logger(names)

    def begin_epoch(self):
        self.train_stats.reset()
        self.valid_stats.reset()
        # Get the time when an epoch begins.
        self.start_time = time.time()

    def after_loss(self):
        stats = self.train_stats if self.in_train else self.valid_stats
        with torch.no_grad(): stats.accumulate(self.run)

    # For printing out summary stats of after each epoch.
    def after_epoch(self):
        stats = [str(self.epoch)] # Begin with the epoch's number.
        for o in [self.train_stats, self.valid_stats]:
            # And nicely format all other metrics to be displayed.
            stats += [f'{v:.6f}' for v in o.avg_stats]
        # Also ensure total duration of is displayed.
        stats += [format_time(time.time() - self.start_time)]
        # Use Learner's self.logger function to display the metrics.
        # print is the default but we will pass fastprogress'
        # write() function to self.logger.
        self.logger(stats)

class ProgressBarCallback(Callback):
    _order=-1
    def begin_fit(self):
        self.master_bar = master_bar(range(self.epochs))
        self.master_bar.on_iter_begin()
        # Callback class stores the Learner() object under self.run
        self.run.logger = partial(self.master_bar.write, table=True)

    def after_fit(self): self.master_bar.on_iter_end()
    def after_batch(self): self.progress_bar.update(self.iter)
    def begin_epoch(self): self.set_progress_bar()
    def begin_validate(self): self.set_progress_bar()

    def set_progress_bar(self):
        self.progress_bar = progress_bar(self.dl, parent=self.master_bar, auto_update=False)
        self.master_bar.update(self.epoch)
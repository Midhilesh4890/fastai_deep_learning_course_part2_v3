
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/08_data_block_my_reimplementation.ipynb

from exports.nb_07a import *

import PIL, os, mimetypes
Path.ls = lambda x: list(x.iterdir())

image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))

def setify(o):
    return o if isinstance(o, set) else set(listify(o))

def _get_file_paths(path, files, extensions=None):
    path = Path(path)
    result = [path/file for file in files if not file.startswith('.')
              and ((f'.{file.split(".")[-1].lower()}' in extensions)
                   or (not extensions))]
    return result

def get_file_paths(path, extensions=None, recurse=False, include=None):
    path = Path(path)
    extensions = setify(extensions)
    extensions = {e.lower() for e in extensions}
    if recurse:
        results = []
        for i, (path, dirnames, filenames) in enumerate(os.walk(path)):
            if include is not None and i==0:
                dirnames[:] = [o for o in dirnames if o in include]
            else:
                dirnames[:] = [o for o in dirnames if not o.startswith('.')]
            results += _get_file_paths(path, filenames, extensions)
        return results
    else:
        files = [o.name for o in os.scandir(path) if o.is_file()]
        return _get_file_paths(path, files, extensions)


# Takes a list of transform functions and applies them, in
# order, to an item in the dataset.
def apply_transforms(x, funcs, *args, order_key='_order', **kwargs):
    key = lambda o: getattr(o, order_key, 0)
    for func in sorted(listify(funcs), key=key):
        x = func(x, **kwargs)
    return x

class ItemList(ListContainer):
    def __init__(self, items, path='.', tfms=None):
        super().__init__(items)
        self.path, self.tfms = Path(path), tfms

    def __repr__(self):
        return f'{super().__repr__()}\nPath: {self.path}'

    def new(self, items, cls=None):
        if cls is None: cls = self.__class__
        return cls(items, self.path, tfms=self.tfms)

    def get(self, i): return i

    def _get(self, i): return apply_transforms(self.get(i), self.tfms)

    def __getitem__(self, idx):
        result = super().__getitem__(idx)
        if isinstance(result, list):
            return [self._get(o) for o in result]
        return self._get(result)

class ImageList(ItemList):
    @classmethod
    def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs):
        if extensions is None: extensions = image_extensions
        return cls(get_file_paths(path, extensions, recurse=recurse, include=include), path, **kwargs)

    def get(self, file_name):
        return PIL.Image.open(file_name)

class Transform():
    _order = 0

# No need to specify _order attribute for MakeRGB.
# We want to use default value of 0.
class MakeRGB(Transform):
    def __call__(self, item): return item.convert('RGB')

class ResizeFixed(Transform):
    _order = 10
    def __init__(self, size):
        if isinstance(size, int): size = (size, size)
        self.size = size

    def __call__(self, item): return item.resize(self.size, PIL.Image.BILINEAR)

def to_byte_tensor(item):
    result = torch.ByteTensor(torch.ByteStorage.from_buffer(item.tobytes()))
    w, h = item.size
    return result.view(h,w,-1).permute(2,0,1)

# Set order to ensure we convert to byte tensor
# after resizing.
to_byte_tensor._order=20

def to_float_tensor(item): return item.float().div_(255.)
to_float_tensor._order=30

def _split_by_function(items, func):
    mask = [func(o) for o in items]
    # `None` values are filtered out
    falses = [o for o, m in zip(items, mask) if m==False]
    trues  = [o for o, m in zip(items, mask) if m==True]
    return falses, trues

def grandparent_splitter(filename, valid_name='valid', train_name='train'):
    grandparent = filename.parent.parent.name
    return True if grandparent==valid_name else False if grandparent==train_name else None

class SplitData():
    def __init__(self, train, valid):
        self.train, self.valid = train, valid

    def __getattr__(self, k): return getattr(self.train, k)

    # This method necessary if we ever want to pickle this
    # SplitData class and load it without any recursion errors.
    def __setstate__(self, data:Any):
        self.__dict__.update(data)

    @classmethod
    def split_by_function(cls, image_list, func):
        lists = map(image_list.new, _split_by_function(image_list.items, func))
        return cls(*lists)

    def __repr__(self):
        return f'{self.__class__.__name__}\nTrain: {self.train}\nValid: {self.valid}\n'

# Convenience wrapper function that can take a ImageList() object
# and returns a SplitData() object that has split the dataset
# according to the information provided by the function that's
# passed via the 'func' parameter.
def split_by_function(image_list_object, func):
    return SplitData.split_by_function(image_list_object, func)

from collections import OrderedDict

# Helper method to return a list of unique values
def uniqueify(x, sort=False):
    result = list(OrderedDict.fromkeys(x).keys())
    if sort: result.sort()
    return result

# General class for all Processors
class Processor():
    def process(self, items): return items

# Processor class to convert categories to unique integer indices
class CategoryProcessor(Processor):
    def __init__(self): self.vocab=None

    def __call__(self, items):
        # Category "vocab" defined on first use
        if self.vocab is None:
            self.vocab = uniqueify(items)
            # Create mapping of each class to an index
            self.otoi  = {v:k for k,v in enumerate(self.vocab)}
        return [self.process_one_item(o) for o in items]

    # Get the index corresponding to a class label
    def process_one_item(self, item): return self.otoi[item]

    def deprocess(self, idxs):
        assert self.vocab is not None
        return [self.deprocess_one_item(idx) for idx in idxs]

    # Get the class label corresponding to one index
    def deprocess_one_item(self, idx): return self.vocab[idx]


# The parent directory of each Imagenette image file's path
# contains the class label. This helper method returns
# that label for us.
def parent_labeler(filename): return filename.parent.name

# Private method which will be used to apply the above
# parent_labeler function to each item in the dataset.
# Will generate an ItemList containing the class label of
# of each item.
def _label_by_function(dataset, func, cls=ItemList):
    return cls([func(o) for o in dataset.items], path=dataset.path)

class LabeledData():
    def process(self, imagelist, processor):
        return imagelist.new(apply_transforms(imagelist.items, processor))

    def __init__(self, x, y, x_processor=None, y_processor=None):
        self.x, self.y = self.process(x, x_processor), self.process(y, y_processor)
        self.x_processor, self.y_processor = x_processor, y_processor

    def __repr__(self): return f'{self.__class__.__name__}\nx: {self.x}\ny: {self.y}\n'

    def __getitem__(self, idx): return self.x[idx], self.y[idx]

    def __len__(self): return len(self.x)

    # Get the actual image object of any item in dataset
    def x_obj(self, idx): return self.obj(self.x, idx, self.x_processor)

    # Get the actual class label of any item in dataset
    def y_obj(self, idx): return self.obj(self.y, idx, self.y_processor)

    def obj(self, items, idx, processors):
        isint = isinstance(idx, int) or (isinstance(idx, torch.LongTensor) and not idx.ndim)
        item = items[idx]
        for processor in reversed(listify(processors)):
            item = processor.deprocess_one_item(item) if isint else processor.deprocess(item)
        return item

    @classmethod
    def label_by_function(cls, imagelist, func, x_processor=None, y_processor=None):
        return cls(imagelist, _label_by_function(imagelist, func), x_processor=x_processor, y_processor=y_processor)

# Convenience wrapper function that can take a SplitData() object
# and apply a processor to its train and valid sets. The
# processor labels according to the info provided by a labeler
# function (func).
def label_by_function(split_data_object, func, x_processor=None, y_processor=None):
    train = LabeledData.label_by_function(split_data_object.train, func, x_processor=x_processor, y_processor=y_processor)
    valid = LabeledData.label_by_function(split_data_object.valid, func, x_processor=x_processor, y_processor=y_processor)
    return SplitData(train, valid)

def show_image(im, figsize=(3,3)):
    plt.figure(figsize=figsize)
    plt.axis('off')
    plt.imshow(im.permute(1,2,0))

class DataBunch():
    def __init__(self, train_dl, valid_dl, channels_in=None, channels_out=None):
        self.train_dl, self.valid_dl = train_dl, valid_dl
        self.channels_in, self.channels_out = channels_in, channels_out

    @property
    def train_ds(self): return self.train_dl.dataset

    @property
    def valid_ds(self): return self.valid_dl.dataset

def databunchify(split_data_object, bs, channels_in=None, channels_out=None, **kwargs):
    dataloaders = get_dls(split_data_object.train, split_data_object.valid, bs, **kwargs)
    return DataBunch(*dataloaders, channels_in=channels_in, channels_out=channels_out)

SplitData.to_databunch = databunchify

ImageList.to_split = split_by_function
SplitData.to_label = label_by_function

def normalize_channels(x, mean, std):
    return (x-mean[...,None,None]) / std[...,None,None]

_means = tensor([0.4879, 0.4740, 0.4307])
_std_devs = tensor([0.2814, 0.2831, 0.3079])

norm_imagenette = partial(normalize_channels, mean=_means.cuda(), std=_std_devs.cuda())

import math

def prev_pow_2(x): return 2**math.floor(math.log2(x))

def get_cnn_layers(data, n_outs, layer, **kwargs):

    def build_layer(n_in, n_out, stride=2):
        return layer(n_in, n_out, ks=3, stride=stride, **kwargs)

    L1 = data.channels_in
    L2 = prev_pow_2(L1 * 3 * 3)
    layers = [build_layer(L1  , L2  , stride=1),
              build_layer(L2  , L2*2, stride=2),
              build_layer(L2*2, L2*4, stride=2)]
    n_outs = [L2*4] + n_outs
    layers += [build_layer(n_outs[i], n_outs[i+1]) for i in range(len(n_outs) - 1)]
    layers += [nn.AdaptiveAvgPool2d(1),
               Lambda(flatten),
               nn.Linear(n_outs[-1], data.channels_out)]
    return layers

def get_cnn_model(data, n_outs, layer, **kwargs):
    return nn.Sequential(*get_cnn_layers(data, n_outs, layer, **kwargs))

def get_learn_run(n_outs, data, lr, layer, cbs=None, opt_func=None, **kwargs):
    model = get_cnn_model(data, n_outs, layer, **kwargs)
    init_cnn(model)
    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)

def model_summary(run, learn, data, find_all=False):
    xb, yb = get_batch(data.valid_dl, run)
    device = next(learn.model.parameters()).device
    xb, yb = xb.to(device), yb.to(device)
    modules = find_modules(learn.model, is_lin_layer) if find_all else learn.model.children()
    f = lambda hook, module, inp, outp: print(f"{module}\n{outp.shape}\n")
    with ForwardHooks(modules, f) as hooks: learn.model(xb)
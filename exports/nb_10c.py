
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/10c_fp16_my_reimplementation.ipynb

from exports.nb_10b import *

import apex.fp16_utils as fp16

def get_master(optimizer, flat_master=False):
    model_params = [[param for param in param_group if param.requires_grad] for param_group in optimizer.param_groups]
    if flat_master:
        master_params = []
        for param_group in model_params:
            master_param_group = parameters_to_vector([param.data.float() for param in param_group])
            master_param_group = torch.nn.Parameter(master_param_group, requires_grad=True)
            if master_param_group.grad is None:
                master_param_group = master_param_group.new(*master_param_group.size())
            master_params.append(master_param_group)
    else:
        master_params = [[param.clone().float().detach() for param in param_group] for param_group in model_params]
        for param_group in master_params:
            for param in param_group: param.requires_grad_(True)
    return model_params, master_params

def to_master_grads(model_param_groups, master_param_groups, flat_master:bool=False)->None:
    for (model_param_group, master_param_group) in zip(model_param_groups, master_param_groups):
        fp16.model_grads_to_master_grads(model_param_group, master_param_group, flat_master=flat_master)

def to_model_params(model_param_groups, master_param_groups, flat_master:bool=False)->None:
    for (model_param_group, master_param_group) in zip(model_param_groups, master_param_groups):
        fp16.master_params_to_model_params(model_param_group, master_param_group, flat_master=flat_master)

def test_overflow(x):
    grad_sum = float(x.float().sum())
    return (grad_sum == float('inf') or grad_sum == float('-inf') or grad_sum != grad_sum)

def grad_overflow(param_groups):
    for param_group in param_groups:
        for param in param_group:
            if param.grad is not None:
                test_overflow(param.grad.data)

class MixedPrecision(Callback):
    _order = 99
    def __init__(self, loss_scale=512, flat_master=False, dynamic=True, max_loss_scale=2.**24,
                 div_factor=2., scale_wait=500):
        assert torch.backends.cudnn.enabled, 'Mixed-precision training requires that the cuDNN be installed.'
        self.flat_master = flat_master
        self.dynamic = dynamic
        self.max_loss_scale = max_loss_scale
        self.div_factor = div_factor
        self.scale_wait = scale_wait
        self.loss_scale = max_loss_scale if dynamic else loss_scale

    def begin_fit(self):
        # Helper 1: Convert model (except for any batchnorm layers) to FP16:
        self.run.model = fp16.convert_network(self.model, dtype=torch.float16)

        # Helper 2: Creating a FP32 master copy of parameter weights
        self.model_param_groups, self.master_param_groups = get_master(self.opt, self.flat_master)
        # To place those FP32 master copy param groups inside the runner:
        self.run.opt.param_groups = self.master_param_groups

        # To count number of iterations without gradient overflow occurring.
        if self.dynamic: self.count = 0

    def after_fit(self): self.model.float()

    # Convert inputs to FP16 before forward pass
    def begin_batch(self): self.run.xb = self.run.xb.half()

    # Convert preds to FP32 so that loss can be computed in FP32
    def after_pred(self): self.run.pred = self.run.pred.float()

    # Scale loss to avoid gradient underflow (FP16 seeing non-zero grads as zero)
    def after_loss(self): self.run.loss *= self.loss_scale

    def after_backward(self):
        # Helper 5: check whether gradient overflow is occurring.
        if self.dynamic and grad_overflow(self.model_param_groups):
            # Divide loss scale factor by the div_factor (usually 2.)
            # if there is gradient overflow.
            self.loss_scale /= self.div_factor
            # We just zero out the gradients and skip the weight
            # update step if there are NaN gradients.
            self.model.zero_grad()
            return True

        # Helper 3: Copy FP16 gradients (resulting from backprop) to FP32 master.
        to_master_grads(self.model_param_groups, self.master_param_groups, self.flat_master)

        # Also unscale gradients so that weight update can be computed in FP32.
        for param_group in self.master_param_groups:
            for param in param_group:
                if param.grad is not None: param.grad.div_(self.loss_scale)

        # Check if we can double the loss scale factor (if it's been
        # long enough since a gradient overflow occurred).
        if self.dynamic:
            self.count += 1
            if self.count == self.scale_wait:
                self.count = 0
                self.loss_scale *= self.div_factor

    def after_step(self):
        # Be sure to zero out all gradients after the weight update step.
        self.model.zero_grad()

        # Helper 4: Copy updated weights from FP32 master back to FP16 model,
        #           so that next iteration's forward pass can be done in FP16.
        to_model_params(self.model_param_groups, self.master_param_groups, self.flat_master)
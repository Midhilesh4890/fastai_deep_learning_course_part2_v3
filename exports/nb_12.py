
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/12_text_my_reimplementation.ipynb

from exports.nb_11a import *

def read_file(fn):
    with open(fn, 'r', encoding = 'utf8') as f: return f.read()

class TextList(ItemList):
    @classmethod
    def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs):
        return cls(get_file_paths(path, extensions, recurse=recurse, include=include), path, **kwargs)

    def get(self, i):
        if isinstance(i, Path): return read_file(i)
        return i

import spacy, html


# Special tokens
UNK = "xxunk"
PAD = "xxpad"
BOS = "xxbos"
EOS = "xxeos"
TK_REP = "xxrep"
TK_WREP = "xxwrep"
TK_UP = "xxup"
TK_MAJ = "xxmaj"

default_special_tokens = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]


# Clean up messy text:

def sub_br(t):
    """Replaces <br /> with \n"""
    re_br = re.compile(r'<\s*br\s*/?>', re.IGNORECASE)
    return re_br.sub("\n", t)

def spec_add_spaces(t):
    """Add spaces around / and # characters"""
    return re.sub(r'([/#])', r' \1 ', t)

def rm_useless_spaces(t):
    """Replace two or more consecutive spaces with a single space"""
    return re.sub(' {2,}', ' ', t)

def fixup_text(x):
    """Various messy things Jeremy and Sylvain have seen in documents"""
    re1 = re.compile(r'  +')
    x = x.replace('#39;', "'").replace('amp;', '&').replace('#146;', "'").replace(
        'nbsp;', ' ').replace('#36;', '$').replace('\\n', "\n").replace('quot;', "'").replace(
        '\\"', '"').replace('<unk>', UNK).replace(' @.@ ', '.').replace(
        ' @-@ ', '-').replace('\\', ' \\ ')
    return re1.sub(' ', html.unescape(x))


# Special pre-tokenization applied before spaCy:

def replace_rep(t):
    """
    Replace four or more repetitions of the same character.
    For example, "cccc" becomes "TK_REP 4 c"
    """
    def _replace_rep(m: Collection[str]) -> str:
        c, cc = m.groups()
        return f' {TK_REP} {len(cc) + 1} {c} '
    re_rep = re.compile(r'(\S)(\1{3,})')
    return re_rep.sub(_replace_rep, t)

def replace_wrep(t):
    """
    Replace four or more repetitions of the same word.
    For example, "word word word" becomes "TK_WREP 3 word"
    """
    def _replace_wrep(m: Collection[str]) -> str:
        c, cc = m.groups()
        return f' {TK_WREP} {len(cc.split()) + 1} {c} '
    re_wrep = re.compile(r'(\b\w+\W+)(\1{3,})')
    return re_wrep.sub(_replace_wrep, t)

default_pre_rules = [sub_br, spec_add_spaces, rm_useless_spaces,
                     fixup_text, replace_rep, replace_wrep]


# Special tokenization applied after spaCy:

def replace_all_caps(x):
    """
    Replace tokens composed of all uppercase characters with lowercase,
    and insert the token `TK_UP` so that it directly precedes the now
    uppercase token.
    """
    result = []
    for t in x:
        if t.isupper() and len(t) > 1:
            result.append(TK_UP)
            result.append(t.lower())
        else: result.append(t)
    return result

def replace_capitalized(x):
    """
    Replaces any capitalized tokens (words where the first character only is
    uppercase) with an uncapitalized version, and inserts the token `TK_MAJ` so
    that it directly now precedes the now uncapitalized token.
    """
    result = []
    for t in x:
        if t == '': continue
        if t[0].isupper() and len(t) > 1 and t[1:].islower(): result.append(TK_MAJ)
        result.append(t.lower())
    return result

def add_eos_bos(x):
    """
    Adds the `BOS` and `EOS` tokens at the beginning and end, respectively,
    of each stream/list (previously an individual movie review) of tokens.
    """
    return [BOS] + x + [EOS]

default_post_rules = [replace_all_caps, replace_capitalized, add_eos_bos]


from spacy.symbols import ORTH
from concurrent.futures import ProcessPoolExecutor

def parallel(func, arr, max_workers=4):
    if max_workers < 2:
        results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))
    else:
        with ProcessPoolExecutor(max_workers = max_workers) as ex:
            return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr)))
    if any([o is not None for o in results]): return results


class TokenizeProcessor(Processor):
    def __init__(self, lang='en', chunksize=2000, pre_rules=None, post_rules=None, max_workers=4):
        self.chunksize, self.max_workers = chunksize, max_workers
        # Use spaCy's default tokenizing rules.
        self.tokenizer = spacy.blank(lang).tokenizer
        # And add a few special ones of our own to take place
        # before and after spaCy is run.
        for w in default_special_tokens:
            self.tokenizer.add_special_case(w, [{ORTH: w}])
        self.pre_rules  = default_pre_rules if pre_rules is None else pre_rules
        self.post_rules = default_post_rules if post_rules is None else post_rules

    def process_chunk(self, args):
        i, chunk = args
        chunk = [apply_transforms(t, self.pre_rules) for t in chunk]
        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]
        docs = [apply_transforms(t, self.post_rules) for t in docs]
        return docs

    def __call__(self, items):
        tokens = []
        if isinstance(items[0], Path): items = [read_file(i) for i in items]
        chunks = [items[i: i + self.chunksize] for i in (range(0, len(items), self.chunksize))]
        tokens = parallel(self.process_chunk, chunks, max_workers=self.max_workers)
        return sum(tokens, [])

    def process_one_item(self, item): return self.process_chunk([tokens])[0]

    def deprocess(self, tokens): return [self.deprocess_one_item(token) for token in tokens]

    def deprocess_one_item(self, token): return " ".join(token)


import collections

class NumericalizeProcessor(Processor):
    def __init__(self, vocab=None, max_vocab=60000, min_freq=2):
        self.vocab, self.max_vocab, self.min_freq = vocab, max_vocab, min_freq

    def __call__(self, items):
        # The overall vocab is defined on first use.
        if self.vocab is None:
            freq = Counter(p for o in items for p in o)
            self.vocab = [o for o, c in freq.most_common(self.max_vocab) if c >= self.min_freq]
            for o in reversed(default_special_tokens):
                if o in self.vocab: self.vocab.remove(o)
                self.vocab.insert(0, o)

            # Ensure max_vocab not exceeded:
            if len(self.vocab) > self.max_vocab:
                self.vocab = self.vocab[:self.max_vocab]

        if getattr(self, 'otoi', None) is None:
            # Create mapping of each vocab item to an index
            # Note: 'otoi' means "object to int"
            self.otoi = collections.defaultdict(int, {v: k for k, v in enumerate(self.vocab)})
        return [self.process_one_item(o) for o in items]

    def process_one_item(self, item): return [self.otoi[o] for o in item]

    def deprocess(self, idxs):
        assert self.vocab is not None
        return [self.deprocess_one_item(idx) for idx in idxs]

    def deprocess_one_item(self, idx): return [self.vocab[i] for i in idx]


class LM_Dataset():
    def __init__(self, data, bs=64, bptt=70, shuffle=False):
        self.data, self.bs, self.bptt, self.shuffle = data, bs, bptt, shuffle
        corpus_len = sum([len(t) for t in data.x]) # Number words/tokens in the training set
        self.per_batch_chunk_size = corpus_len // bs
        self.batchify()

    # The length of the dataset. Specifically, the total number of
    # bptt sequences that will be processed by all batches across
    # all training iterations.
    #
    # Why subtract 1 from each batch's total chunk of the corpus?
    # This is to ensure that the final input token seen by each
    # batch at end of the final iteration has a corresponding target
    # to predict!
    def __len__(self): return ((self.per_batch_chunk_size - 1) // self.bptt) * self.bs

    def __getitem__(self, idx):
        source = self.batched_data[idx % self.bs]
        seq_idx = (idx // self.bs) * self.bptt
        # Return a tuple of:
        # 1. Inputs: <bs> number of batches, each containing
        #            <bptt> tokens.
        # 2. Targets: Same shape and tokens as inputs, but
        #             advances each token's index by one.
        #             For a token at a given index in an input
        #             bptt sequence, the corresponding target
        #             needs to be the word in the corpus text
        #             that comes next.
        return source[seq_idx: seq_idx + self.bptt], source[seq_idx + 1: seq_idx + self.bptt + 1]

    def batchify(self):
        texts = self.data.x
        if self.shuffle: texts = texts[torch.randperm(len(texts))]
        stream = torch.cat([tensor(t) for t in texts])
        self.batched_data = stream[:self.per_batch_chunk_size * self.bs].view(self.bs, self.per_batch_chunk_size)


def get_language_model_dls(train_ds, valid_ds, bs, bptt, **kwargs):
    # Return training and validation dataloaders for training a language model.
    return (DataLoader(LM_Dataset(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),
            # Can use a larger bs for val because only doing inference.
            DataLoader(LM_Dataset(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))

def lm_databunchify(split_data_object, bs, bptt, **kwargs):
    return DataBunch(*get_language_model_dls(split_data_object.train, split_data_object.valid, bs, bptt, **kwargs))

SplitData.to_lm_databunch = lm_databunchify

TextList.to_split = split_by_function


from torch.utils.data import Sampler

class SortSampler(Sampler):
    def __init__(self, data_source, key): self.data_source, self.key = data_source, key
    def __len__ (self): return len(self.data_source)
    def __iter__(self):
        return iter(sorted(list(range(len(self.data_source))),
                           key = self.key,
                           reverse = True))


class SortishSampler(Sampler):
    def __init__(self, data_source, key, bs):
        self.data_source, self.key, self.bs = data_source, key, bs

    def __len__(self) -> int: return len(self.data_source)

    def __iter__(self):
        idxs = torch.randperm(len(self.data_source))
        megabatches_of_idxs = [idxs[i: i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]
        megabatches_of_sorted_idxs = torch.cat([tensor(sorted(m, key=self.key, reverse=True)) for m in megabatches_of_idxs])
        batches = [megabatches_of_sorted_idxs[i: i+self.bs] for i in range(0, len(megabatches_of_sorted_idxs), self.bs)]
        idx_batch_with_longest_text = torch.argmax(tensor([self.key(batch[0]) for batch in batches]))
        batches[0], batches[idx_batch_with_longest_text] = batches[idx_batch_with_longest_text], batches[0]
        longest_batch = batches[0]
        short_batch = batches[-1]
        shuffled_batch_idxs = torch.randperm(len(batches) - 2)
        sorted_batches = torch.cat([batches[i+1] for i in shuffled_batch_idxs]) if len(batches) > 1 else LongTensor([])
        sorted_batches = torch.cat([longest_batch, sorted_batches, short_batch])
        return iter(sorted_batches)


def pad_collate(samples, pad_idx=1, pad_first=False):
    # pad_idx is the index of the padding token in our corpus' vocab.
    longest_len = max([len(s[0]) for s in samples])
    result = torch.zeros(len(samples), longest_len).long() + pad_idx
    for i, s in enumerate(samples):
        length_sample = len(s[0])
        if pad_first: result[i, -length_sample:] = LongTensor(s[0])
        else:         result[i, :length_sample ] = LongTensor(s[0])
    return result, tensor([s[1] for s in samples])


def get_text_clas_dls(train_ds, valid_ds, bs, **kwargs):
    train_sampler = SortishSampler(train_ds.x, key = lambda t: len(train_ds.x[t]), bs=bs)
    # We never randomly sort the order of validation documents
    valid_sampler = SortSampler(   valid_ds.x, key = lambda t: len(valid_ds.x[t]))
    return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),
            DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))

def text_clas_databunchify(split_data_object, bs, **kwargs):
    return DataBunch(*get_text_clas_dls(split_data_object.train, split_data_object.valid, bs, **kwargs))

SplitData.to_text_clas_databunch = text_clas_databunchify


def dropout_mask(x, size, p):
    return x.new(*size).bernoulli_(1-p).div_(1-p)


class RNNDropout(nn.Module):
    def __init__(self, p = 0.5):
        super().__init__()
        self.p = p

    def forward(self, x):
        # No dropout if not in training.
        if not self.training or self.p == 0.: return x
        m = dropout_mask(x.data, (1, x.size(1), x.size(2)), self.p)
        return x * m


import warnings

# The name of the hidden layer in a PyTorch LSTM cell
# whose weights we wish to apply dropout to.
WEIGHT_HH = 'weight_hh_l0' # Note: this is the letter "l" and zero "0"
                           #       it's NOT the number ten "10"

class WeightDropout(nn.Module):
    def __init__(self, module, p=[0.], layer_names=[WEIGHT_HH]):
        super().__init__()
        self.module, self.p, self.layer_names = module, p, layer_names

        # Makes copies of the weights of the layers
        # to which we'll apply dropout.
        for layer in self.layer_names:
            # First, get the layer.
            w = getattr(self.module, layer)
            # Second, make a clone of the layer's weight parameters and
            # store in a duplicate layer whose name ends in "_raw"
            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))
            # Third, the hacky part: run the original layer weights
            # through a dropout layer, but don't actually apply dropout.
            # We do this so we can replace this dropout layer's "output"
            # the outputs we'll get from running cloned "_raw" version of
            # the layer through dropout during training.
            self.module._parameters[layer] = F.dropout(w, p=self.p, training=False)

    def _set_weights(self):
        for layer in self.layer_names:
            # Get the cloned layer weights.
            raw_w = getattr(self, f'{layer}_raw')
            # Run them through dropout and replace the original layer
            # weights with these dropped-out weights.
            self.module._parameters[layer] = F.dropout(raw_w, p=self.p, training=self.training)

    def forward(self, *args):
        self._set_weights()
        with warnings.catch_warnings():
            # In order to avoid the "weights aren't flattened" warning.
            warnings.simplefilter("ignore")
            return self.module.forward(*args)


class EmbeddingDropout(nn.Module):
    def __init__(self, emb_layer, p):
        super().__init__()
        self.emb_layer, self.p = emb_layer, p
        self.pad_idx = self.emb_layer.padding_idx
        if self.pad_idx is None: self.pad_idx = -1

    def forward(self, words, scale=None):
        # If dropout is applied:
        if self.training and self.p != 0:
            size = (self.emb_layer.weight.size(0), 1)
            mask = dropout_mask(self.emb_layer.weight.data, size, self.p)
            w = self.emb_layer.weight * mask
        # If dropout isn't applied:
        else: w = self.emb_layer.weight
        if scale: w.mul_(scale)
        return F.embedding(words, w, self.pad_idx, self.emb_layer.max_norm,
                           self.emb_layer.norm_type, self.emb_layer.scale_grad_by_freq,
                           self.emb_layer.sparse)


def to_detach(h):
    """
    Detaches a tensor h from its history.
    """
    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)

class AWD_LSTM(nn.Module):
    """
    Based on approaches recommended by Smerity et. al.
    in https://arxiv.org/abs/1708.02182
    """

    initrange = 0.1

    def __init__(self, vocab_size, emb_size, n_hid, n_layers, pad_token,
                 embed_p=0.1, hidden_p=0.2, input_p=0.6, weight_p=0.5):
        super().__init__()
        self.bs, self.emb_size, self.n_hid, self.n_layers = 1, emb_size, n_hid, n_layers

        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=pad_token)
        self.emb.weight.data.uniform_(-self.initrange, self.initrange)
        self.emb_dropout = EmbeddingDropout(self.emb, embed_p)

        self.lstms = [nn.LSTM(emb_size if layer == 0 else n_hid,
                             (n_hid if layer != n_layers - 1 else emb_size), 1,
                             batch_first = True) for layer in range(n_layers)]
        # Use WeightDropout for LSTM gates' hidden layers.
        self.lstms = nn.ModuleList([WeightDropout(lstm, weight_p) for lstm in self.lstms])
        # Use RNNDropout on inputs to & outputs of each LSTM layer.
        # LSTM layer outputs can be thought of as the "hidden layer
        # outputs" of the overall RNN network.
        self.rnn_input_dropout = RNNDropout(input_p)
        self.rnn_hidden_dropouts = nn.ModuleList([RNNDropout(hidden_p) for layer in range(n_layers)])

    def _new_empty_layer(self, layer):
        """
        Get one brand new, empty version of a given layer.
        Returns a tuple of two tensors, each sized to store
        the same number of weight parameters originally held
        in the layer. One empty tensor is for the weights;
        the other is for their gradients.
        """
        nh = self.n_hid if layer != self.n_layers - 1 else self.emb_size
        empty_param_tensor = next(self.parameters()).new(1, self.bs, nh).zero_()
        return (empty_param_tensor, empty_param_tensor)

    def reset(self):
        """
        Reset the states of all hidden layers.
        """
        self.hidden = [self._new_empty_layer(layer) for layer in range(self.n_layers)]

    def forward(self, input):
        bs, _ = input.size()
        if bs != self.bs:
            self.bs = bs
            self.reset()
        raw_output = self.rnn_input_dropout(self.emb_dropout(input))
        new_hidden, raw_outputs, outputs = [], [], []
        for i, (lstm_layer, rnn_hidden_dropout) in enumerate(zip(self.lstms, self.rnn_hidden_dropouts)):
            raw_output, new_h = lstm_layer(raw_output, self.hidden[i])
            # Store each layer's hidden state:
            new_hidden.append(new_h)
            # Store each layer's non-dropped-out output:
            raw_outputs.append(raw_output)
            if i != self.n_layers - 1:
                raw_output = rnn_hidden_dropout(raw_output)
            # And store each layer's output after dropout
            # has been applied to it:
            outputs.append(raw_output)
        # Goal with following line is two-fold:
        # 1. Detach layers' hidden states after forward pass,
        #    so that we remove them from computation graph
        #    and don't perform backprop on them.
        # 2. But also save them so that we can use them as inputs
        #    to our LSTM layers on the next forward pass.
        self.hidden = to_detach(new_hidden)
        # raw_outputs = with no dropout; outputs = with dropout
        return raw_outputs, outputs


class LinearDecoder(nn.Module):
    def __init__(self, n_out, n_hid, output_p, encoder_embed=None, bias=True):
        super().__init__()
        self.output_dropout = RNNDropout(output_p)
        self.decoder = nn.Linear(n_hid, n_out, bias=bias)
        if bias: self.decoder.bias.data.zero_()
        # A good idea to use vocab embedding matrix to init decoder's weights.
        if encoder_embed: self.decoder.weight = encoder_embed.weight
        else: init.kaiming_uniform_(self.decoder.weight)

    def forward(self, input):
        raw_outputs, outputs = input
        # Apply dropout to AWD-LSTM's final layer output
        # (the output that came through the AWD-LSTM module's
        #  "dropout" path).
        output = self.output_dropout(outputs[-1]).contiguous()
        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))
        # Return first the predictions, but also the non-dropped-out
        # as well as the dropped-out layer outputs of AWD-LSTM.
        return decoded, raw_outputs, outputs


class SequentialRNN(nn.Sequential):
    """
    This sequential module can pass a reset
    call to all layers that can be reset.
    """
    def reset(self):
        for c in self.children():
            if hasattr(c, 'reset'): c.reset()


def get_language_model(vocab_size, embed_size, n_hid, n_layers, pad_token,
                       output_p=0.4, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5,
                       tie_embed_weights_to_decoder=True, bias=True):

    rnn_encoder = AWD_LSTM(vocab_size, embed_size, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,
                           hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)
    encoder_embed = rnn_encoder.emb if tie_embed_weights_to_decoder else None
    return SequentialRNN(rnn_encoder, LinearDecoder(vocab_size, embed_size, output_p, encoder_embed=encoder_embed,
                                                    bias=bias))

class GradientClipping(Callback):
    def __init__(self, clip=None): self.clip = clip
    def after_backward(self):
        if self.clip: nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)


class RNNTrainer(Callback):
    def __init__(self, α, β): self.α, self.β = α, β

    def after_pred(self):
        # Save RNN encoder's layer 'outputs' (have had dropout) and
        # 'raw_outputs' (didn't have dropout) to calculate AR and TAR
        # after prediction loss has been calculated:
        self.raw_out, self.out = self.pred[1], self.pred[2]
        # Use linear decoder's activation prediction outputs to
        # calculate loss:
        self.run.pred = self.pred[0]

    def after_loss(self):
        # AR
        if self.α != 0.: self.run.loss += self.α * self.out[-1].float().pow(2).mean()
        # TAR
        if self.β != 0.:
            h = self.raw_out[-1]
            # Calculate the difference between RNN encoder's final layer
            # activations (non-dropout path) at timestep t + 1 and
            # timestep t.
            next_timestep_activations = h[:, 1:]
            preceding_timestep_activations = h[:, :-1]
            diff = next_timestep_activations - preceding_timestep_activations
            if len(h) > 1: self.run.loss += self.β * diff.float().pow(2).mean()

    def begin_epoch(self):
        # Shuffle the texts at the beginning of each epoch.
        if hasattr(self.dl.dataset, 'batchify'): self.dl.dataset.batchify()


def cross_entropy_flat(input, target):
    bs, seq_length = target.size()
    return F.cross_entropy(input, target.flatten())

def accuracy_flat(input, target):
    bs, seq_length = target.size()
    return accuracy(input, target.flatten())


def lm_splitter(model):
    layer_groups = []
    for i in range(len(model[0].lstms)):
        layer_groups.append(nn.Sequential(model[0].lstms[i], model[0].rnn_hidden_dropouts[i]))
    layer_groups += [nn.Sequential(model[0].emb, model[0].emb_dropout, model[0].rnn_input_dropout, model[1])]
    return [list(o.parameters()) for o in layer_groups]


from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence


class AWD_LSTM_pad(nn.Module):
    """
    Based on approaches recommended by Smerity et. al.
    in https://arxiv.org/abs/1708.02182
    """

    initrange = 0.1

    def __init__(self, vocab_size, emb_size, n_hid, n_layers, pad_token,
                 embed_p=0.1, hidden_p=0.2, input_p=0.6, weight_p=0.5):
        super().__init__()
        self.bs, self.emb_size, self.n_hid, self.n_layers = 1, emb_size, n_hid, n_layers
        self.pad_token = pad_token

        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=pad_token)
        self.emb.weight.data.uniform_(-self.initrange, self.initrange)
        self.emb_dropout = EmbeddingDropout(self.emb, embed_p)

        self.lstms = [nn.LSTM(emb_size if layer == 0 else n_hid,
                             (n_hid if layer != n_layers - 1 else emb_size), 1,
                             batch_first = True) for layer in range(n_layers)]
        # Use WeightDropout for LSTM gates' hidden layers.
        self.lstms = nn.ModuleList([WeightDropout(lstm, weight_p) for lstm in self.lstms])
        # Use RNNDropout on inputs to & outputs of each LSTM layer.
        # LSTM layer outputs can be thought of as the "hidden layer
        # outputs" of the overall RNN network.
        self.rnn_input_dropout = RNNDropout(input_p)
        self.rnn_hidden_dropouts = nn.ModuleList([RNNDropout(hidden_p) for layer in range(n_layers)])

    def _new_empty_layer(self, layer):
        """
        Get one brand new, empty version of a given layer.
        Returns a tuple of two tensors, each sized to store
        the same number of weight parameters originally held
        in the layer. One empty tensor is for the weights;
        the other is for their gradients.
        """
        nh = self.n_hid if layer != self.n_layers - 1 else self.emb_size
        empty_param_tensor = next(self.parameters()).new(1, self.bs, nh).zero_()
        return (empty_param_tensor, empty_param_tensor)

    def reset(self):
        """
        Reset the states of all hidden layers.
        """
        self.hidden = [self._new_empty_layer(layer) for layer in range(self.n_layers)]

    def forward(self, input):
        bs, padded_doc_length = input.size()

        # Make a mask that indicates location of weights of each
        # padding token in all batches.
        pad_mask = (input == self.pad_token)
        unpadded_doc_lengths = padded_doc_length - pad_mask.long().sum(dim=1)

        # Remove any empty (only have padding tokens) batches:
        n_empty_batches = (unpadded_doc_lengths == 0).sum()
        if n_empty_batches > 0:               # If any batches are empty, those batches
            input = input[:-n_empty_batches]  # will come at the end of the inputs.
            unpadded_doc_lengths = unpadded_doc_lengths[:-n_empty_batches]
            self.hidden = [(h[0][:,:input.size(0)], h[1][:,:input.size(0)]) for h in self.hidden]

        raw_output = self.rnn_input_dropout(self.emb_dropout(input))
        new_hidden, raw_outputs, outputs = [], [], []
        for i, (lstm_layer, rnn_hidden_dropout) in enumerate(zip(self.lstms, self.rnn_hidden_dropouts)):
            # Pack embedding layer outputs so that padding will be
            # ignored when they're fed into the LSTM layers:
            raw_output = pack_padded_sequence(raw_output, unpadded_doc_lengths, batch_first=True)
            # Run through LSTM layer
            raw_output, new_h = lstm_layer(raw_output, self.hidden[i])
            # Unpack LSTM layer outputs, adding padding tokens back to
            # weights (padding tokens' weights will all be 0.0000)
            raw_output = pad_packed_sequence(raw_output, batch_first=True)[0]

            # Store LSTM layer's hidden state:
            new_hidden.append(new_h)
            # Store LSTM layer's non-dropped-out output:
            raw_outputs.append(raw_output)
            # Run LSTM layer's output through dropout
            if i != self.n_layers - 1:
                raw_output = rnn_hidden_dropout(raw_output)
            # And store LSTM layer's output after dropout
            # has been applied to it:
            outputs.append(raw_output)
        # Goal with following line is two-fold:
        # 1. Detach layers' hidden states after forward pass,
        #    so that we remove them from computation graph
        #    and don't perform backprop on them.
        # 2. But also save them so that we can use them as inputs
        #    to our LSTM layers on the next forward pass.
        self.hidden = to_detach(new_hidden)
        # raw_outputs = with no dropout; outputs = with dropout
        return raw_outputs, outputs, pad_mask


def get_batchnorm_dropout_linear(n_in, n_out, bn=True, p=0., activation=None):
    layers = [nn.BatchNorm1d(n_in)] if bn else []
    if p != 0: layers.append(nn.Dropout(p))
    layers.append(nn.Linear(n_in, n_out))
    if activation is not None: layers.append(activation)
    return layers


class PoolingLinearClassifier(nn.Module):
    """
    Create a linear classifier that uses concat pooling.
    """

    def __init__(self, layers, dropouts):
        super().__init__()
        model_layers = []
        activations = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]
        for n_in, n_out, p, act in zip(layers[:-1], layers[1:], dropouts, activations):
            model_layers += get_batchnorm_dropout_linear(n_in, n_out, p=p, activation=act)
        self.layers = nn.Sequential(*model_layers)

    def forward(self, input):
        raw_outputs, outputs, pad_mask = input
        output = outputs[-1]
        lengths = output.size(1) - pad_mask.long().sum(dim=1)
        avg_pool = output.masked_fill(pad_mask[:,:,None], 0).sum(dim=1)
        avg_pool.div_(lengths.type(avg_pool.dtype)[:, None])
        max_pool = output.masked_fill(pad_mask[:,:,None], -float('inf')).max(dim=1)[0]
        x = torch.cat([output[torch.arange(0, output.size(0)), lengths - 1], max_pool, avg_pool], 1)
        x = self.layers(x)
        return x


# Just a helper function to create a padding mask for a bptt-sized
# chunk of a complete text document:
def pad_tensor(t, bs, val=0.):
    if t.size(0) < bs:
        return torch.cat([t, val + t.new_zeros(bs - t.size(0), *t.shape[1:])])
    return t


class DocumentEncoder(nn.Module):
    def __init__(self, module, bptt, pad_idx=1):
        super().__init__()
        self.bptt, self.module, self.pad_idx = bptt, module, pad_idx

    def concat(self, tensors, bs):
        return [torch.cat([pad_tensor(t[i], bs) for t in tensors], dim=1) for i in range(len(tensors[0]))]

    def forward(self, input):
        bs, padded_doc_length = input.size()
        self.module.bs = bs
        self.module.reset()
        raw_outputs, outputs, pad_masks = [], [], []
        for i in range(0, padded_doc_length, self.bptt):
            raw_output, output, pad_mask = self.module(input[:,i: min(i + self.bptt, padded_doc_length)])
            pad_masks.append(pad_tensor(pad_mask, bs, 1))
            raw_outputs.append(raw_output)
            outputs.append(output)
        return self.concat(raw_outputs, bs), self.concat(outputs, bs), torch.cat(pad_masks, dim=1)


def get_text_classifier(vocab_size, embed_size, n_hid, n_layers, n_out, pad_token, bptt,
                        output_p=0.4, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5,
                        clas_layers=None, clas_dropouts=None):
    """
    Creates an AWD-LSTM text document classifier model.
    """
    rnn_encoder = AWD_LSTM_pad(vocab_size, embed_size, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,
                               hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)
    doc_encoder = DocumentEncoder(rnn_encoder, bptt)
    if clas_layers is None: clas_layers = [50]
    if clas_dropouts is None: clas_dropouts = [0.1] * len(clas_layers)
    clas_layers = [3 * embed_size] + clas_layers + [n_out]
    clas_dropouts = [output_p] + clas_dropouts
    return SequentialRNN(doc_encoder, PoolingLinearClassifier(clas_layers, clas_dropouts))


def lang_class_splitter(model):
    enc = model[0].module
    layer_groups = [nn.Sequential(enc.emb, enc.emb_dropout, enc.rnn_input_dropout)]
    for i in range(len(enc.lstms)): layer_groups.append(nn.Sequential(enc.lstms[i], enc.rnn_hidden_dropouts[i]))
    layer_groups.append(model[1])
    return [list(o.parameters()) for o in layer_groups]
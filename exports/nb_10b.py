
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/10b_mixup_label_smoothing_my_reimplementation.ipynb

from exports.nb_10 import *

class NoneReduce():
    def __init__(self, loss_func):
        self.loss_func, self.old_reduction = loss_func, None

    def __enter__(self):
        # If the chosen loss function has a reduction, we want to
        # hold off on using it until we're ready to compute the
        # mixed up image's composite loss. So we temporarily change
        # its reduction to 'none'. Only applicable when in training.
        if hasattr(self.loss_func, 'reduction'):
            self.old_reduction = getattr(self.loss_func, 'reduction')
            setattr(self.loss_func, 'reduction', 'none')
            return self.loss_func
        else: return partial(self.loss_func, reduction='none')

    def __exit__(self, type, value, traceback):
        # Restore the loss function's reduction back to its original
        # specification (if we had changed it to 'none' during training).
        if self.old_reduction is not None:
            setattr(self.loss_func, 'reduction', self.old_reduction)

from torch.distributions.beta import Beta

# Helper function to add to a tensor new, empty axes at specified dimensions.
def unsqueeze(input, dims):
    for dim in listify(dims): input = torch.unsqueeze(input, dim)
    return input

def linear_comb(a, b, t): return t*a + (1-t)*b

# Helper function to finally apply our loss reduction during training
# when we're good and ready -- after we've computed loss1 and loss2
# for each mixed up image.
def reduce_loss(loss, reduction='mean'):
    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss

class MixUp(Callback):
    _order = 90 # Ensure that this runs after normalization and Cuda callbacks
    def __init__(self, α:float=0.4): self.distrib = Beta(tensor([α]), tensor([α]))

    def begin_fit(self):
        # Keep track of the loss function that we *had* been using
        # before training began, which wouldn't have yet had its
        # reduction switched to 'none'.
        self.old_loss_func = self.run.loss_func
        # Set the training loop loss function to this class'
        # self.loss_func() method (defined just below).
        self.run.loss_func = self.loss_func

    def begin_batch(self):
        # Only perform mixup during training.
        if not self.in_train: return
        t = self.distrib.sample((self.yb.size(0),)).squeeze().to(self.xb.device)
        t = torch.stack([t, 1-t],1)
        self.t = unsqueeze(t.max(1)[0], (1,2,3))
        # Generate a list of random indices; includes each index in
        # the batch.
        shuffle = torch.randperm(self.yb.size(0)).to(self.xb.device)
        xb1, self.yb1 = self.xb[shuffle], self.yb[shuffle]
        # Create mixed up images
        self.run.xb = linear_comb(self.xb, xb1, self.t)

    # After training, switch the loss function back to the
    # version of the loss function that doesn't have it's
    # reduction attribute switched to 'none'.
    def after_fit(self): self.run.loss_func = self.old_loss_func

    def loss_func(self, pred, yb):
        if not self.in_train: return self.old_loss_func(pred, yb)
        with NoneReduce(self.old_loss_func) as loss_func:
            loss1 = loss_func(pred, yb)
            loss2 = loss_func(pred, self.yb1)
        loss = linear_comb(loss1, loss2, self.t)
        return reduce_loss(loss, getattr(self.old_loss_func, 'reduction', 'mean'))

class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, ε:float=0.1, reduction='mean'):
        super().__init__()
        self.ε, self.reduction = ε, reduction

    def forward(self, output, target):
        num_classes = output.size()[-1]
        log_preds = F.log_softmax(output, dim=-1)
        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)
        nll = F.nll_loss(log_preds, target, reduction=self.reduction)
        return linear_comb(loss/num_classes, nll, self.ε)